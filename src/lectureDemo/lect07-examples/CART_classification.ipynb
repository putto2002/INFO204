{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad54005e-73f0-4c1a-9610-a1712fc90c01",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab9fa6-2e98-4204-a9f9-48a7d77b514b",
   "metadata": {},
   "source": [
    "As discussed in class, the k-Nearest Neighbours method works by exploiting an existing database of _labelled_ observations. To predict the (unknown) value of a new observation, we embed it in the space of the existing observations measure the distance between the new instance and the existing observations, and then use the labels of the $k$ nearest observations to determine the prediction. In the context of classification, the prediction is the majority label found within the labels of the $k$ nearest neighbours.\n",
    "\n",
    "To work with k-Nearest Neighbours in Python, we an make use of the existing [numpy](https://numpy.org/) and [scikit-learn](https://scikit-learn.org/) libraries. Let's start by importing them (and a [matplotlib](https://matplotlib.org/) for some plotting of the results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cdcc4-8b6b-4d43-b81a-c895c4274e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d445e6-ad96-4bf7-ac43-645eb0bf94c9",
   "metadata": {},
   "source": [
    "## Our \"Training\" Data (this is identical to Lecture 5)\n",
    "\n",
    "We'll also need some data to act as the historical observations from our problem. For this example, we'll just make some up (but usually, you'd be given this data). In this case, we (the people doing the work) actually know the underlying function $f(X)$, but from the modelling perspective this function is not known:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf508bc3-590f-4dca-8602-b874a1d04210",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the true underlying \"generating\" function of our problem, in this case\n",
    "## we are using it to separate our data into two classes \"a\" and \"b\" depending\n",
    "## upon where in a 2-D space an instance sits.\n",
    "##\n",
    "## AS WITH REGRESSION, LET'S PRETEND THAT WE DON'T KNOW THIS ONE :)\n",
    "def f(X):\n",
    "    t = 0.444*(X[:, 0] + 0.5)**2 + 0.5*np.sin(np.pi * X[:, 0])\n",
    "    return np.where(X[:, 1] > t, 1, 0)\n",
    "class_labels = np.array([ 'a', 'b' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc4e91-4816-499e-9f49-3edd18e9444c",
   "metadata": {},
   "source": [
    "Now, we will use this function to generate some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83fbfe-8bac-4657-94fe-217e63e44241",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234) ## notice the fixed seed for reproducability\n",
    "\n",
    "n_points = 50\n",
    "X_train = rng.uniform(-1, 1, size=(n_points, 2))\n",
    "y_train = f(X_train)\n",
    "X_train += rng.normal(0, 0.05, size=X_train.shape) ## let's just add a little noise to our data to make it interesting\n",
    "\n",
    "## we'll also generate some \"test\" data use this to test the shape of our learned function shortly\n",
    "n_test = 50\n",
    "xx1, xx2 = np.meshgrid(np.linspace(-1.1, 1.1, n_test), np.linspace(-1.1, 1.1, n_test))\n",
    "X = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "y = f(X)\n",
    "Z = y.reshape(xx1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07dd38-39e9-4e62-90ae-be444aa56dfe",
   "metadata": {},
   "source": [
    "Let's take a look at the data (and underlying generating function) before moving on to modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1db66-0b81-4a92-bed6-0b63d3b2fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-1, 1, 500)\n",
    "t = 0.444*(a + 0.5)**2 + 0.5*np.sin(np.pi * a)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], color=np.where(y_train==1, 'orange', 'cornflowerblue'))\n",
    "plt.plot(a, t, color='black', label='True Underlying Class Separator - f(X)')\n",
    "plt.title('Our Sampled Training Data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e059c64-7428-4ae8-9444-74aa6c12bbe9",
   "metadata": {},
   "source": [
    "Note that the black line ($f(X)$) serves as the point of separation between two classes (although we've added a little noise to our instances, so there's a couple that manage to sneak over to opposite sides of this boundary). Remember, the function that this line represents is not known to k-Nearest Neighbours - its job is to estimate this function from available training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c70861-2519-4024-aecc-f858b7f24c99",
   "metadata": {},
   "source": [
    "## Applying CART\n",
    "\n",
    "Now that we have a training set of data, we can move onto modelling. We do this by instantiating a DecisionTreeClassifier model, calling the fit function, and then making predictions from the resulting model with the predict function. When building a CART model, we need to specify the minimum split size of a node (called <code>min_samples_split</code> in scikit-learn)) - here we will examine this process for a range of <code>min_samples_split</code> values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f2789-a001-4ee2-88e1-458de6e3c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(2, 3, figsize=(16,10))\n",
    "for (i, k) in enumerate([ 2, 5, 10, 15, n_points, 2*n_points ]):\n",
    "    mdl = DecisionTreeClassifier(random_state=0, min_samples_split=k)\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = mdl.predict(X)\n",
    "    Z = y_pred.reshape((n_test, n_test))\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    r = i // 3\n",
    "    c = i % 3\n",
    "    ax[r, c].contourf(xx1, xx2, Z, cmap=ListedColormap(['cornflowerblue', 'orange']), alpha=0.2)\n",
    "    ax[r, c].scatter(X_train[:, 0], X_train[:, 1], color=np.where(y_train == 1, 'orange', 'cornflowerblue'), label='Sampled Training Data')\n",
    "    ax[r, c].plot(a, t, color='black', label='True Underlying Class Separator - f(X)')\n",
    "    ax[r, c].set_title(\"CART Performance, minsplit={}, Accuracy={}\".format(k, np.round(acc, 2)))\n",
    "    ax[r, c].set_xlabel('x1')\n",
    "    ax[r, c].set_ylabel('x2')\n",
    "    ax[r, c].set_xlim(-1.1, 1.1)\n",
    "    ax[r, c].set_ylim(-1.1, 1.1)\n",
    "    ax[r, c].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5427ad9-0534-456d-b498-d71888c40379",
   "metadata": {},
   "source": [
    "In each plot, the shading represents the class that would be predicted in that region of the input space. As can be seen, as <code>min_samples_split</code> increases, the complexity of the tree reduces and produces simpler (more rectangular) decision boundaries (where the class prediction changes from one label to the other).\n",
    "\n",
    "In this case, we used a training set of 50 observations - you may wish to modify the code above so that a larger traning set is used (this can be done by modifying the n_points variable to be larger, say 250 observations). Try this and note the impact that this has on estimating the class boundaries.\n",
    "\n",
    "## Visualising the CART model\n",
    "In the above examples, CART was run on the same data six times with different values for <code>min_samples_split</code>, which resulted in six different trees being produced. One of the most valuable aspect of CART is that, for reasonably small trees, the model is highly interpretable (i.e., we can read the tree as a series of rules, and we can visualise the tree as a hierarchical series of splitting rules. To visualise a tree, we can use the <code>plot_tree</code> function from scikit-learn. For example, the six models created in the previous cell can be visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79a865-17a6-4616-8405-dc1f4c613acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(2, 3, figsize=(16, 10))\n",
    "for (i, k) in enumerate([ 2, 5, 10, 15, n_points, 2*n_points ]):\n",
    "    mdl = DecisionTreeClassifier(random_state=0, min_samples_split=k)\n",
    "    mdl.fit(X_train, y_train)\n",
    "    acc = accuracy_score(f(X), mdl.predict(X))\n",
    "\n",
    "    r = i // 3\n",
    "    c = i % 3\n",
    "    plot_tree(mdl, ax=ax[r, c], feature_names=[ 'X1', 'X2' ], rounded=True, label='none', proportion=False, impurity=False, filled=True)\n",
    "    ax[r, c].set_title(\"CART Performance, minsplit={}, accuracy={}\".format(k, np.round(acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90569fef-8932-498d-913b-ac78d932a325",
   "metadata": {},
   "source": [
    "We interpret the trees as follows: the leaf nodes show the number of training instances that matched the splitting criteria up to this point (top number), along with the mean of the response of these training instances (the bottom number). The interior nodes show three things: the splitting rule at this point (top row), the number of matching training instances at this point in the tree (second row), and the mean of the response of these training instances (bottom row). We read the tree as a series of if-then-else rules. For example, the CART tree resulting from <code>min_samples_split</code>=5 can be read as follows:\n",
    "<pre>\n",
    "if X1 <= 0.211 then\n",
    "    if X2 <= -0.034 then\n",
    "        if X2 <= -0.461 then\n",
    "            prediction = Class 1 (confidence = 9/9)\n",
    "        else\n",
    "            prediction = Class 1 (confidence = 2/4)\n",
    "    else\n",
    "        prediction = Class 2 (confidence = 14/14)\n",
    "else\n",
    "    prediction = Class 1 (confidence = 23/23)\n",
    "</pre>\n",
    "\n",
    "Note that, as <code>min_samples_split</code> _increases_, the tree complexity _decreases_, right up to the point where the tree collapses into a single node.\n",
    "\n",
    "## Examining the effect of <code>min_samples_split</code>\n",
    "The main hyperparameter for CART is the minimum node split size <code>min_samples_split</code>. In the previous step, we looked at an arbitrary set of possible values for <code>min_samples_split</code> - let's now be a little more rigorous and examine the performance of CART over a more thorough sweep of values of <code>min_samples_split</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda58201-31a4-4ce8-8742-fc72c103d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split = np.arange(2, n_points + 1)\n",
    "split_loss = []\n",
    "for k in all_split:\n",
    "    mdl = DecisionTreeClassifier(random_state=0, min_samples_split=k)\n",
    "    mdl.fit(X_train, y_train)\n",
    "    split_loss.append(accuracy_score(f(X), mdl.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c487f-081d-43c3-8a38-82d8e66b4463",
   "metadata": {},
   "source": [
    "Finally, we can plot the loss against the neighbourhood size to see how $k$ influences the behaviour of the algorithm and the resulting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f5534-1d39-4aa6-9915-401b15d91797",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_split = np.argmax(split_loss)\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.plot(all_split, split_loss)\n",
    "plt.scatter(all_split[best_split], split_loss[best_split], color='#ce2227', label=\"Best Accuracy, minsplit={}\".format(all_split[best_split]))\n",
    "plt.xlabel('minsplit')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim((0,1))\n",
    "plt.title('CART Performance for minsplit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b6871-ff02-4135-a845-c087a0f4bf6a",
   "metadata": {},
   "source": [
    "Note here that we are measuring accuracy, so a larger score is better (unlike in regression, where we were measuring error, so a lower score was better).\n",
    "\n",
    "In this result, we can see that (for this sample of training data!) the best accuracy can be achieved with <code>min_samples_split</code> set to 1.\n",
    "\n",
    "As mentioned above, you should repeat this step with a larger training set to see if there is any noteworthy change in behaviour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
